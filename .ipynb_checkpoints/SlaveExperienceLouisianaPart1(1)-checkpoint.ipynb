{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776e7b47-dd2b-418c-a17f-c7c509cd067f",
   "metadata": {},
   "source": [
    "Last article, I gave my basic reasons for pursuing a research project into the changing demographics of the slave trade in the Americas. This was very broad, and very vague. However, now I am about to plunge into locating the necessary data to reconstruct the history I wish to reconstruct. And already I am running into technical issues. \n",
    "\n",
    "The archive I want to use was created 22 years ago, back when Microsoft was the preeminent provider of consumer computer products. Everything is saved as a .dbf database file, with .sav, .sps and other files used to port it to an SPSS application. I had never heard of SPSS before this project. It's a proprietary system currently owned by IBM and used heavily in the GIS community. Opening this type of layered database file in a modern environment is complicated. Microsoft has a deprecated product, Visual FoxPro, and of course there are paid products. However, I want to use a modern Jupyter notebook with Python, pandas and other standard data science tools.\n",
    "\n",
    "I turned to the great oracle, Google, to find out how to open a .dbf file in Jupyter. There is a fairly well known package, PySal, that seemed ideal. Unfortunately, I am using Python 3.8. Pysal is dependent on a package called Rasterio, which in turn is dependent on a set of C libraries called GDAL. And here is where I ran into issues. GDAL does not, for some reason, edit its path environment variables on a Windows machine with Python 3.8. There are open issues with several development teams, but all are quite recent (the past year or so) and the issue is not yet fixed. While I'd love to use a Linux machine, at the moment I'm stuck with Windows.\n",
    "\n",
    "So I am going to have to sacrifice the convenience of Pysal and turn to a more hacked together solution. Python comes with a dbfread library. It is going to take some work to get dbfread into Pandas, but hopefully not too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7172926a-631f-40e3-9329-320309174275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dbfread import DBF\n",
    "\n",
    "# Need this later on for data cleaning\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Chart visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb477f82-3e6f-4682-adb8-d6e26891e34e",
   "metadata": {},
   "source": [
    "Above, I imported Pandas. Pandas is perhaps the most popular data science package for Python and is extremely useful. I've used some of its competitors, especially Turi, and Pandas is both better documented and more supported. It does have some strange quirks, such as its odd use of zero indexing, but overall is a fun package to use.\n",
    "\n",
    "Notice I'm also using dbfread. I'm using a script form the documentation at https://dbfread.readthedocs.io/en/latest/exporting_data.html. Please be nice to me processor gods, I just want a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebae8a97-f821-4124-b87b-7577cfce8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbf = DBF('SLAVE.DBF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1123a0-d04a-4fc0-bfbc-a60b1d9a2af7",
   "metadata": {},
   "source": [
    "Let's try to convert the .dbf database into a Pandas dataframe. I'm going to use Pandass' inbuilt DataFrame function because it will continue to be updated with Pandas' development. It is important to note that dbfread is only a somewhat maintained library. After parsing the source code, I noticed a lot of development is still needed. This would come back to bite me as I worked with this project. Per dbfread's documentation, we also need to pass an iterable form of the dbf file we created above. Below is the single line of code that nearly destroyed my sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d0a99f-f44f-4c0e-8c6f-da388d77e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iter(dbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d396c15-f576-49af-98b3-5e46a73a02b1",
   "metadata": {},
   "source": [
    "Originally, this errored. And it errored in the most bizarre way. It was throwing errors about float numbers and integers with full stops. I was very confused and google didn't help much. In fact, most people just ended up editing their data file. I do not have the ability to open a dbf file, nor did I really want to open a file, edit it however many times, and close it again. So, I opened the source code of dbfread to the line that triggered the exception. Below I have a copy of the exception, lovingly recreated for you after I fixed it. Notice all the nonsense about integers and floats and b'.'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d02239-67e6-4b60-9452-30ecce73e21d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "c:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dbfread\\field_parser.py in parseN(self, field, data)\n",
    "    179         try:\n",
    "--> 180             return int(data)\n",
    "    181         except ValueError:\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: b'.'\n",
    "\n",
    "***Edited for clarity***\n",
    "\n",
    "c:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dbfread\\field_parser.py in parseN(self, field, data)\n",
    "    184             else:\n",
    "    185                 # Account for , in numeric fields\n",
    "--> 186                 return float(data.replace(b',',b'.'))\n",
    "    187                 '''\n",
    "    188                 if isinstance(data, float) or len(data) >= 2:\n",
    "\n",
    "ValueError: could not convert string to float: b'.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e8827-5c1e-4bc0-8d7b-4e39fca73c6f",
   "metadata": {},
   "source": [
    "As I said, googling this error only turned up more people with the same issue. Since I couldn't use the package that everyone else seemed to prefer (PySal!) I decided to wade into the source code. Perhaps somewhere in the field_parser.py file I'd find a clue to fixing my code.\n",
    "\n",
    "I didn't. Instead, I found the reason the code errored in the first place. Here's some code taken direcly out of a \n",
    "\n",
    "    def parseN(self, field, data):\n",
    "        \"\"\"\n",
    "        Parse numeric field (N)\n",
    "\n",
    "        Returns int, float or None if the field is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In some files * is used for padding.\n",
    "        data = data.strip().strip(b'*')\n",
    "        try:\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            if not data.strip():\n",
    "                return None\n",
    "            else:\n",
    "                # Account for , in numeric fields\n",
    "                return float(data.replace(b',',b'.'))\n",
    "                \n",
    "Folks, that was the entirety of the exception handling. The default case was, simply, to replace all commas with full stops and then try to convert whatever monstrosity resulted into a float. It is understandable Python didn't really want to do this. By this logic, if the number 1,234.56 was passed to the parseN method, the result would be 1.234.56. This is, obviously, neither a float nor an integer. It is an unholy creation of man. I decided to fix this.\n",
    "\n",
    "First, I decided the final case should be the float NaN. NaN stands for, in the surreally linear logic of computer scientists, *N*ot *A* *N*umber. Get it? NaN...Not A Number. This meant that, if the code was absolutely sure we were dealing with numbers, but it kept mangling whatever it was passed, it should just insert the Pythonic equivalent of a shrug. This is probably not the best design, but short of refactoring the whole dbfreads library I thought it would work as a stopgap. The code now read:\n",
    "\n",
    "    def parseN(self, field, data):\n",
    "        \"\"\"\n",
    "        Parse numeric field (N)\n",
    "\n",
    "        Returns int, float or None if the field is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In some files * is used for padding.\n",
    "        \n",
    "        data = data.strip().strip(b'*')\n",
    "        try:\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            if not data.strip():\n",
    "                return None\n",
    "            else:\n",
    "                # Account for , in numeric fields\n",
    "                if isinstance(data, float):\n",
    "                    return float(data.replace(b',', b'.')\n",
    "                return float(b\"NaN\")\n",
    "                \n",
    "I don't actually know why the function needs to return a binary code. Perhaps there's a calling function somewhere in dbfreads that only accepts binary. Maybe it's a powermove on the part of the developers, who wanted to remind the computer it only speaks binary. Nonetheless, I am now returning the binary version of the float number \"NaN\".\n",
    "\n",
    "This didn't quite fix my problem. Instead, I still kept getting that error about the binary full stop from earlier. It had moved on a few lines (I was inserting counters at one point to trace where in the file there were errors) but I was still having problems with this function. So I started thinking about when a number might *look* like a float but was, in fact, never meant to be such a thing. Suppose there was a field that said *0,*. This would obviously be corruption or entry error, but hey data is messy. If you called this parseN function on *0,*, it wouldn't convert to an integer, it *would* raise an exception, but would not trigger the replace method. Instead, it would turn into a float \"NaN\". I didn't want that data loss, even if I don't understand the data. So, I should really build a trap for such pieces of data.\n",
    "\n",
    "\n",
    "    def parseN(self, field, data):\n",
    "        \"\"\"\n",
    "        Parse numeric field (N)\n",
    "\n",
    "        Returns int, float or None if the field is empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In some files * is used for padding.\n",
    "        data = data.strip().strip(b'*')\n",
    "\n",
    "        try:\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            if not data.strip():\n",
    "                return None\n",
    "            else:\n",
    "                # Account for , in numeric fields\n",
    "\n",
    "                if len(data) >= 2:\n",
    "                    return float(data.replace(b',',b'.'))\n",
    "                return float(b'NaN')\n",
    "                \n",
    "See my clever little trap? I'm checking to see if the length of the data is over 2. Why? Well, I've already checked if the number is a float. If the data was *0,0* it would already be caught. But the number *1,500.0* still won't be caught. I don't want numbers that are only 1 digit long. They will easily convert to integers, unless the program is trying to pass *','* or some other crazy character as a number. I don't know enought about the program to dig through why it is trying to pass those characters, so I weed them out with the length check. Now, *1,500.0* will get the floating point conversion treatment it deserves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e341b7-d6fa-4b58-ae04-3732316e4435",
   "metadata": {},
   "source": [
    "I also had to make one other little edit. There was a similar problem with a datetime method. However, in this case it looks like the originaly developer forgot to add a .strip() method they clearly meant to. This is because, in the comments, they clearly indicate they intend on testing for both empty spaces *and* spaces full of zeros for a datetime column. Nonetheless, they only strip off the zeros, not the spaces. I added a clause looking for the empty spaces and now it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e469c-01e8-4dc2-b282-42cdea3ce4ca",
   "metadata": {},
   "source": [
    "This is the end of the post. There's still a lot of work to do to make this data accessible and accurate enough for any sort of valid historical conclusions to be drawn. However, it is now loaded, the dbfread source code has been strengthened, and it's time to take a break. Somewhere, distantly, I hear a martini glass clinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18329b8-362a-46de-b674-7342f602c52a",
   "metadata": {},
   "source": [
    "OK, picking up where I left off. It's time to clean this data! To see what I mean, observe the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9a1de0-18c9-4b24-9abe-116c9873679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE        object\n",
       "YEAR            int64\n",
       "DOCNO          object\n",
       "NOTARY         object\n",
       "CODER         float64\n",
       "               ...   \n",
       "ARRIVEDATE     object\n",
       "FROM           object\n",
       "UNBAPT        float64\n",
       "VIA           float64\n",
       "COMMENTS       object\n",
       "Length: 114, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce187f1d-c926-44a0-a107-9d1d2fcf2d29",
   "metadata": {},
   "source": [
    "While there are 114 columns to be concerned about, we need to start at the very beginning. The column DOCDATE should be a datetime object, but notice that Pandas currently only knows that this column is an object. The object dtype indicates some form of string, or a mixed type column. This tricky little clause means we are uncertain if all but one DOCDATE entries is a proper datetime object, and one is broken, or if there is a scattering of non-datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e48973e-89f8-478c-a1e4-60230c84a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOCDATE'] = pd.to_datetime(df['DOCDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8a7c40-4c46-471d-80ee-48e82309ab41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE       datetime64[ns]\n",
       "YEAR                   int64\n",
       "DOCNO                 object\n",
       "NOTARY                object\n",
       "CODER                float64\n",
       "                   ...      \n",
       "ARRIVEDATE            object\n",
       "FROM                  object\n",
       "UNBAPT               float64\n",
       "VIA                  float64\n",
       "COMMENTS              object\n",
       "Length: 114, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1fd2c-90c3-456f-a231-5118972342ae",
   "metadata": {},
   "source": [
    "Surprisingly, I was able to simply convert the DOCDATE column directly to the datetime object in Python. This indicates the DOCDATE column was, in fact, full of dates and not something else. We can continue on like this for all 114 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf31db7-e379-49d4-b623-f055f1c49cfc",
   "metadata": {},
   "source": [
    "I'm about to do something a bit controversial. Essentially, I'm converting my YEAR column, but to an integer and not a date. My reason for this is pretty simple. I don't actually want to use the year column AS a date. Dates are for things like machine output and other artifacts from the Information Age. I'm interested in a simple number, namely the number of years in the Common Era. I may have to manipulate this data in ways that is more useful as an integer, such as finding how many years elapsed between a slaves sale and resale, etc. While there are probably functions for all of this work, for the sake of clarity I'm converting YEAR into an integer and not a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecf5a9c-c348-47de-9b9f-7ebabdf0845b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1719\n",
       "1         1719\n",
       "2         1719\n",
       "3         1719\n",
       "4         1719\n",
       "          ... \n",
       "100661    1820\n",
       "100662    1820\n",
       "100663    1820\n",
       "100664    1820\n",
       "100665    1820\n",
       "Name: YEAR, Length: 100666, dtype: int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['YEAR'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4039cc0-fdff-420e-8ef3-60a127cf5a86",
   "metadata": {},
   "source": [
    "As fun as it might be to continually change each of these fields by hand, I'm the lazy sort of data analyst who believes computers were invented so I could lounge in the pool. It's time to put some good old Python into use. I know there are 114 columns. I know that I want *most* of those columns to be strings. This is because there is a lot of data that is best understood as text. Columns like NOTARY and FROM are clearly meant to be a personal name and a placename. However, there are a few that should *NOT* be strings. The column ARRIVEDATE, for instance, should always be a datetime. \n",
    "Other columns should be mostly an integer or float, except for some sort of value indicating an unknown status. The column AGE, for instance, should never be a string unless the answer is 'UNKNOWN'. While the practice of owning humans as chattel was dehumanizing in the extreme, the resale of humans did create a need for strict record keeping. Most slave ages would be known to a relatively sure degree, if they were born on a plantation. A slave newly trafficked, or perhaps resold from a more chaotic or fraudulent area, would have an unknown age. So what I need to do now is find all of the columns that *look* like they should be non-strings. If their current pandas dtype is 'object', this indicates mixed types such as a number and a string, a float and an int, or some other form of mixed up data.\n",
    "So, first, I need to identify which columns have mixed types. Below I wrote some code to iterate over the dataframe, select all of the object dtypes, and return a list. I know in introductory programming courses I hated the section on list comprehensions, because they seemed to be confusing, but in a Jupyter notebook like this it really is quite handy to have all of the code on a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d8108c-d87b-4786-af52-c2955a3134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = [col for col in df.columns if df[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd420a-b5cb-4fe5-ac40-c96855bd82d2",
   "metadata": {},
   "source": [
    "Now that I know precisely what columns have an object dtype, I need to inspect the list manually. I don't know of an automatic way to figure out what columns shouldn't have strings. Perhaps when the singularity comes, this will be a side effect. Of course, it doesn't seem like Skynet or whatever Facebook means by Meta will care much for human history. Though they might quite like the idea of studying slavery as a how-to manual...\n",
    "\n",
    "So, should the Singularity come, I'm putting in my bid as Advisor to the Singularity on Slavery. In the meantime, I'm going to try to continue rehumanizing people who were dehumanized. And fighting the good fight against corrupt data. And drink as many ciders as possible.\n",
    "\n",
    "To return to the point, let's see what columns might potentially need our attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718f6626-3b04-4f89-8083-8fb140503777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOCNO',\n",
       " 'NOTARY',\n",
       " 'DATEINV',\n",
       " 'DATESALE',\n",
       " 'PARISH',\n",
       " 'ESTATE_OF',\n",
       " 'FIRSTNAME',\n",
       " 'ESTATE',\n",
       " 'SELLER',\n",
       " 'FIRST1',\n",
       " 'BUYER',\n",
       " 'FIRST2',\n",
       " 'NAMEXPLAIN',\n",
       " 'NAME',\n",
       " 'SKILLS',\n",
       " 'CHARACTER',\n",
       " 'SICK',\n",
       " 'SPELL',\n",
       " 'GROUPMEMB',\n",
       " 'INVCUR',\n",
       " 'SALECUR',\n",
       " 'FAMILY',\n",
       " 'SPNATMOM',\n",
       " 'MATENAME',\n",
       " 'SPELNAMATE',\n",
       " 'SPNADAD',\n",
       " 'SPNAGM',\n",
       " 'SPNAGRPA',\n",
       " 'CAPTAIN',\n",
       " 'SHIP',\n",
       " 'ARRIVEDATE',\n",
       " 'FROM',\n",
       " 'COMMENTS']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804b728-307a-40e7-9f9c-f7590a697ff4",
   "metadata": {},
   "source": [
    "From this list, I have identified only a few columns that clearly require an integer, float or date. 'DOCNO' is, I'm fairly certain, going to be a numeric code for the documents. It is also possible it is an alphanumeric code, but given the small(ish) number of documents and age of the project I suspect it is numeric. However, ARRIVEDATE, AGE, DATESALE and DATEINV all seem like they should not be strings. There are a few columns that I'm just not sure about. What is SALECUR? Or INVCUR? Since I don't know, I'll have to inspect the columns manually after I'm done this work. But for now, it is import to get as many columns as possible finished.\n",
    "Below I will make a list of columns I don't think are strings. I will call this, creatively, not_strings. I can add to it later using list methods, and delete if I'm mistaken. I am also using a list comprehension to pull out the not_strings entries from the object_col list. I will call this new list, again creatively, strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3c7496-7d6b-4be8-894f-a6754ca8408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_strings = ['DOCNO','ARRIVEDATE','AGE','DATESALE','DATEINV']\n",
    "strings = [entry for entry in object_col if entry not in not_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d505a4-1f85-4b69-bc55-329d7a81de9d",
   "metadata": {},
   "source": [
    "Now that I have all of these lists floating around, it is time to actually convert things to strings. It took me awhile to figure out the current method to do this. Pandas was created as a numerical and scientific computing library. It is only recently that a better string dtype has been added. Using methods that seem obvious (to_string()) does not change the dtype. Instead, you need to use a very specific set of formats. Scouring the internet, again, is not very helpful. The documentation is your best source, look at it here\" https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html. Although the string datatype was first implemented in Pandas 1.0.0, there are StackOverflow posts listing incorrect methods for this conversion. ONLY use the ones in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca731d3-1591-4481-9274-64dace0fdf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_to_strings(strings):\n",
    "    '''\n",
    "    Takes a list of columns that should be converted to string type and converts them all.\n",
    "    '''\n",
    "    for entries in strings: #iterate over my list of strings, not the entire Dataframe\n",
    "        df[entries] = df[entries].astype('string')  \n",
    "\n",
    "cols_to_strings(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72f2bd0-c95c-433c-aa6c-9c4a11d02ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE       datetime64[ns]\n",
       "YEAR                   int64\n",
       "DOCNO                 object\n",
       "NOTARY                string\n",
       "CODER                float64\n",
       "                   ...      \n",
       "ARRIVEDATE            object\n",
       "FROM                  string\n",
       "UNBAPT               float64\n",
       "VIA                  float64\n",
       "COMMENTS              string\n",
       "Length: 114, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to make sure I've successfully added strings\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4883f2-051f-4e51-bafc-b0b43dc1797a",
   "metadata": {},
   "source": [
    "As you can see, I have successfully changed the vast majority of the columns to strings. However, I'm not done yet. What about those five columns with mixed types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351e1789-dbc5-47de-9669-54a5d9787e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOCNO', 'ARRIVEDATE', 'AGE', 'DATESALE', 'DATEINV']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495f302-d04d-4924-90bc-ae9e03aee44f",
   "metadata": {},
   "source": [
    "Yeah, those. Let's fix those up. I suspect DOCNO should be a set of integers, but I'm not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7916612b-d54f-49ab-91b8-6f95821c9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               \n",
       "1               \n",
       "2               \n",
       "3               \n",
       "4               \n",
       "           ...  \n",
       "100661      3754\n",
       "100662       223\n",
       "100663       224\n",
       "100664       363\n",
       "100665       363\n",
       "Name: DOCNO, Length: 100666, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849ad4b-3091-49d1-ad0e-6dd49efd4c0a",
   "metadata": {},
   "source": [
    "Looks pretty integery to me. Let's try to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1d6a1b-f9ea-4a6f-afca-a45141e25e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4264/3203796966.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5877\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5878\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     def convert(\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[1;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "df['DOCNO'] = df['DOCNO'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3a9aa-6624-4080-8729-7c2b1f57ace3",
   "metadata": {},
   "source": [
    "Ooooh hooo hooo. An error. What might this beastie be?\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: ''\n",
    "\n",
    "Looks like there are some emtpy spaces and/or someone left a blank in the column. Luckily, this is Pandas. It was made for precisely this moment. I could, theoretically, write a for loop that would replace *''* with, well, anything I want. However, all I really need is to call Pandas' function *replace* and someone much better at optimization will take care of the matter for me.\n",
    "\n",
    "First, I want Pandas to recognize empty spaces as a missing value. Basically, Pandas would really prefer that everything was just numbers. The library was created by some mad scientists, after all, and was never intended for historical research. It doesn't like to recognize things like *None* or a blank space as a missing value. But history is full of messy people doing messy things, so we have to allow as many types of missing values as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a0b5d0-696b-431c-9ae1-367f67b5cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_option(\"mode.use_inf_as_na\")\n",
    "pd.set_option(\"mode.use_inf_as_na\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819093cc-cfc5-4bc5-bd8a-fa5e78285a4f",
   "metadata": {},
   "source": [
    "OK, after some random sampling, it seems like df['DOCNO'] is full of strings that look like numbers to the human eye. Consider the very last such entry: df['DOCNO'][100665]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5c4410-4615-4523-b85e-bb043745fee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   363'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO'][100665]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef8d70-7e14-498b-91f4-e3293692ff4e",
   "metadata": {},
   "source": [
    "I want to remove ALL leading spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1d61130-13b2-41ae-a276-2a5551f1822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'363'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO'][100665].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3e8d8-f4e9-4c9b-a340-95901891c14f",
   "metadata": {},
   "source": [
    "But not just for that one entry. I want to do it for the entire column. There is, however, a slight problem. Observe this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8fd9c3-0fa0-4673-af50-cde00d2e5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fee3f3-ea12-4349-84a8-0b2bf1220508",
   "metadata": {},
   "source": [
    "I want to use the Pandas' replace function to insert NaN into every empty space. The reason for this is quite simple. An empty space may or may not be interpreted as a missing value, but NaN will ALWAYS be interpreted as a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07d2f6a-8f10-42a4-b1bd-eef80197b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOCNO'] = df['DOCNO'].replace('',np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac2b20-471a-4771-8643-bbba9bf1e379",
   "metadata": {},
   "source": [
    "Pandas provides a method to check if I have empty values, as specified by NaN. It is the .isna() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c85ea79-a835-4eb3-b672-5481420f9361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          True\n",
       "1          True\n",
       "2          True\n",
       "3          True\n",
       "4          True\n",
       "          ...  \n",
       "100661    False\n",
       "100662    False\n",
       "100663    False\n",
       "100664    False\n",
       "100665    False\n",
       "Name: DOCNO, Length: 100666, dtype: bool"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO'].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa029f-ff82-4a98-a09e-78a9f0fd904c",
   "metadata": {},
   "source": [
    "OK, now I know that Pandas recognizes my missing values as missing values. What next? Well, let's try converting the DOCNO column to a float. NaN is technically a float value, and while Pandas provides an int64 type that will allow me to use NaN, I'd rather just convert to a float for now. The Pandas methods should also strip all of those leading blank spaces away. Hopefully. If the processor gods are merciful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95adb53d-33f7-46d3-ae6e-c82cba0f2f09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'fol.89'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4264/2039978824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5877\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5878\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     def convert(\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'fol.89'"
     ]
    }
   ],
   "source": [
    "df['DOCNO'] = df['DOCNO'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687ca8b-c090-4c28-82ea-c3c043dad1d9",
   "metadata": {},
   "source": [
    "And...it errored. Why? Well, here is the error.\n",
    "\n",
    "ValueError: could not convert string to float: 'fol.89'\n",
    "\n",
    "This seems to say that I have a string in the column that I didn't know about. Unfortunately, I've already half converted the column to floats, so using string methods doesn't work too well. I'm going to have to sort through the column, look at the type of the individual row, and then find what columns contain anything with 'fol' in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e2b482d-a42a-4d10-94e8-3076ad6aa781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fol.89']\n"
     ]
    }
   ],
   "source": [
    "def mixed_type_search(offending_phrase, col):\n",
    "    '''\n",
    "    Takes a string, the offending phrase, and searches for it in an entire column.\n",
    "    \n",
    "    Returns a list of all variations containing that phrase.\n",
    "    '''\n",
    "    # list of offending phrases\n",
    "    list_of_offense = []\n",
    "    \n",
    "    for elements in df[col]:       \n",
    "        try:\n",
    "            # We can only look at elements that are strings.\n",
    "            if type(elements) == str and offending_phrase in elements and elements not in list_of_offense:\n",
    "                list_of_offense.append(elements)\n",
    "                \n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # This might go wrong. It shouldn't lead to an infinite loop, because the \n",
    "                # string conversion should make the offending value fail the test next go \n",
    "                # around.\n",
    "                \n",
    "                if type(elements) != str:\n",
    "                    mixed_type_search(str(elements),col)\n",
    "                    \n",
    "            except:\n",
    "                print(\"The offending phrase cannot be found due to type incompatibility\")\n",
    "                return False\n",
    "    return list_of_offense\n",
    "\n",
    "print(mixed_type_search('fol','DOCNO'))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab736f6-6648-4f5a-9b6d-c596106b66c7",
   "metadata": {},
   "source": [
    "Based on these results, it seems like there is only one fol (folder? folio? who cares?). It should be replaced with just simply the number 89, provided the number 89 doesn't already exist in the column. We should check that to make sure we aren't going to merge data that should be distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84e2aa78-b922-4901-8eea-60c7113c1999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value is not in column already\n"
     ]
    }
   ],
   "source": [
    "def does_value_already_exist(value, col, sub_value=None):\n",
    "    '''\n",
    "    Takes a value and returns whether the column specified in col already contains\n",
    "    that value. Used to maintain unique data, such as in DOCNO.\n",
    "    \n",
    "    Value is used for longer strings such as 'fol.89'. But I want to check\n",
    "    whether 89 is in the file, but excluding fol.89.\n",
    "    '''\n",
    "    str(value)\n",
    "    str(col)\n",
    "    \n",
    "    # sub_value is optional so the method can be used with just one string\n",
    "    if value in df[col] and sub_value == None:\n",
    "        return True\n",
    "    \n",
    "    # exclusion logic\n",
    "    elif value not in df[col] and sub_value != None:\n",
    "        if sub_value in df[col] :\n",
    "            return True\n",
    "    return \"Value is not in column already\"\n",
    "\n",
    "print(does_value_already_exist('fol.89', 'DOCNO','89'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f368403-b79f-4a51-844a-228ba30ad42c",
   "metadata": {},
   "source": [
    "Based on the results of the method above, I'd say we're ok replacing fol.89 with 89."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2da1a0d-cd31-427c-a16e-ce0bd1f8325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOCNO'] = df['DOCNO'].replace('fol.89',89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47e428-a07b-4bb0-84e8-71c8b9a549df",
   "metadata": {},
   "source": [
    "Let's go back to our original goal of converting all of DOCNO to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b605b73-0d87-4b6a-80f1-8de44cadc799",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'p.252'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4264/2039978824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5877\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5878\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     def convert(\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'p.252'"
     ]
    }
   ],
   "source": [
    "df['DOCNO'] = df['DOCNO'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587ade2-9cbd-44fc-98c9-d6f8d245f116",
   "metadata": {},
   "source": [
    "Good heavens, another error.\n",
    "\n",
    "ValueError: could not convert string to float: 'VIII'\n",
    "\n",
    "This one, again, should be easy to fix. That's just the roman numeral 8. However, we need to check if doc 8 is already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d87d1ec-3904-4fa7-a7c4-c4a0c467e27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Value is not in column already'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "does_value_already_exist('8', 'DOCNO')  # No need for optional parameter, as I \n",
    "                                        # only care if 8 exists as a document \n",
    "                                        # number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4ec77-8ca4-46b4-8a25-a33a3423eae1",
   "metadata": {},
   "source": [
    "OK, I consider that clearance to just replace VIII with 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fdbe7137-dc4e-4e7e-9902-e2cf91fdf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOCNO']= df['DOCNO'].replace('VIII',8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e648a2a-678c-44bb-84f5-0a7082b4dc5b",
   "metadata": {},
   "source": [
    "Time to try a float conversion again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "254392b1-ca59-4f72-a543-fd9a2b45f77e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'p.252'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4264/2039978824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DOCNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5877\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5878\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     def convert(\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'p.252'"
     ]
    }
   ],
   "source": [
    "df['DOCNO'] = df['DOCNO'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e717e7-e55a-4679-aee0-63d523887d7c",
   "metadata": {},
   "source": [
    "And more errors. Honestly, it seems like this column could have been left a string. However, I also think imposing an obvious numbering system on the DOCNO column is important later on. Here's the error.\n",
    "\n",
    "ValueError: could not convert string to float: 'p.252'\n",
    "\n",
    "Let's see if it's around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bdda62a-425e-46f0-b0d9-b16caeff7a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "does_value_already_exist(252, 'DOCNO') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3e6f7-f050-46f1-94c3-96efc23eaa54",
   "metadata": {},
   "source": [
    "Well, that isn't good. There's already a document 252. Plus, that error message looks like a page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a20a8b00-129b-4722-8e48-d6cf15c994fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47281, 'p.252'], [47282, 'p.252'], [47283, 'p.252'], [47284, 'p.252']]\n"
     ]
    }
   ],
   "source": [
    "def search_for_string_in_column_entry(string, col):\n",
    "    '''\n",
    "    Takes a string and searches the given dataframe column for the string in a \n",
    "    cell of data. Then, returns a list of every location where string occurs.\n",
    "    '''\n",
    "    \n",
    "    locations =  [[index, value] for index, value in df[col].items() if str(string) in str(value)]\n",
    "   \n",
    "    return locations\n",
    "\n",
    "print(search_for_string_in_column_entry('.252','DOCNO'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9c7a3-d3d7-4abb-a8d7-8506478f17ec",
   "metadata": {},
   "source": [
    "Based on these results, we can conclude that someone entered p.252 four times. But why? This seems increasingly like a page number. Is there more to the story? Maybe we should take a close look at each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73ad7b91-76f5-4e66-92a9-7689e19d4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCDATE                                     1806-10-21 00:00:00\n",
      "YEAR                                                       1806\n",
      "DOCNO                                                     p.252\n",
      "NOTARY                                       Sp. W. Fla Vol. XI\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Overseer shot & killed slave, mistress (a wido...\n",
      "Name: 47281, Length: 114, dtype: object\n",
      "DOCDATE                                     1806-10-21 00:00:00\n",
      "YEAR                                                       1806\n",
      "DOCNO                                                     p.252\n",
      "NOTARY                                       Sp. W. Fla Vol. XI\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Overseer shot & killed slave, mistress (a wido...\n",
      "Name: 47282, Length: 114, dtype: object\n",
      "DOCDATE                                     1806-10-21 00:00:00\n",
      "YEAR                                                       1806\n",
      "DOCNO                                                     p.252\n",
      "NOTARY                                       Sp. W. Fla Vol. XI\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Overseer shot & killed slave, mistress (a wido...\n",
      "Name: 47283, Length: 114, dtype: object\n",
      "DOCDATE                                     1806-10-21 00:00:00\n",
      "YEAR                                                       1806\n",
      "DOCNO                                                     p.252\n",
      "NOTARY                                       Sp. W. Fla Vol. XI\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Overseer shot & killed slave, mistress (a wido...\n",
      "Name: 47284, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-03-20 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                      p. 1\n",
      "NOTARY                                           SPWFLA Vol. 14\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Will bequeathing 3 infant girl slaves to testa...\n",
      "Name: 47619, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-03-20 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                      p. 1\n",
      "NOTARY                                           SPWFLA Vol. 14\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         0.0\n",
      "COMMENTS      Will bequeathing 3 infant girl slaves to testa...\n",
      "Name: 47620, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-03-20 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                      p. 1\n",
      "NOTARY                                           SPWFLA Vol. 14\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Will bequeathing 3 infant girl slaves to testa...\n",
      "Name: 47621, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-03-20 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                      p. 1\n",
      "NOTARY                                           SPWFLA Vol. 14\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Will bequeathing 3 infant girl slaves to testa...\n",
      "Name: 47622, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-03-20 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                      p. 1\n",
      "NOTARY                                           SPWFLA Vol. 14\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Will bequeathing 3 infant girl slaves to testa...\n",
      "Name: 47623, Length: 114, dtype: object\n",
      "DOCDATE                                     1807-12-19 00:00:00\n",
      "YEAR                                                       1807\n",
      "DOCNO                                                    p. 142\n",
      "NOTARY                                      West Fla Papers XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Power of Atty to Wm Barr & Samuel Power to rec...\n",
      "Name: 49165, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49357, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49358, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49359, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49360, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49361, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49362, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49363, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-01-03 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 222\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      1.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Negro boy,bozal,just bought.Neck broken,brains...\n",
      "Name: 49364, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51120, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51121, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51122, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51123, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51124, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      \"Civilized man.\" Robbed house, burned it, plan...\n",
      "Name: 51125, Length: 114, dtype: object\n",
      "DOCDATE                                     1808-10-11 00:00:00\n",
      "YEAR                                                       1808\n",
      "DOCNO                                                    p. 131\n",
      "NOTARY                                     Sp. W. Fla Vol. XIII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      0.0\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Robbed house, burned it, planned to run away t...\n",
      "Name: 51126, Length: 114, dtype: object\n",
      "DOCDATE                                     1810-08-17 00:00:00\n",
      "YEAR                                                       1810\n",
      "DOCNO                                                    p. 242\n",
      "NOTARY                                      Sp. W. Fla Vol. XII\n",
      "CODER                                                       1.0\n",
      "                                    ...                        \n",
      "ARRIVEDATE                                                 None\n",
      "FROM                                                           \n",
      "UNBAPT                                                      NaN\n",
      "VIA                                                         NaN\n",
      "COMMENTS      Webb loaned this slave to his brother-in-law R...\n",
      "Name: 56819, Length: 114, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for entries in search_for_string_in_column_entry('.','DOCNO'):\n",
    "    print(df.iloc[entries[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adce4fb-9632-4414-a46d-8e7396cf823d",
   "metadata": {},
   "source": [
    "I suspect the DOCNO column, in these few cases, is meant to be a supplement to the Notary column. While I'm not sure what Sp. W. Fla Vol. XI precisely refers to (I'll have to dig into the older project's notes and source documents), it seems like the sort of thing that points to a book, which would incidentally have pages. So I'm going to transfer the DOCNO information to the Notary column.\n",
    "\n",
    "First, create a variable to store the indices AND values we want transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd7f45c5-5771-4e66-82b8-f1b4a837daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_transfer_index_values = search_for_string_in_column_entry('.','DOCNO')\n",
    "need_transfer_index_only = [index for index, values in need_transfer_index_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3563a-f7b1-4587-9993-eceb3912207c",
   "metadata": {},
   "source": [
    "Now transfer the values in need_transfer over to the column 'NOTARY' and insert a -1 in column 'DOCNO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73467952-16c6-4ab5-b509-fd25fb408587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sp. W. Fla Vol. XI'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOTARY'].iloc[47281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f36343b5-6b17-4047-9d4a-b6322e22dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\casti\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "def transfer_and_replace_col(new_col, old_col, new_value, indices):\n",
    "    '''\n",
    "    Transfers the values in old_col into new_col, replacing old_col with the new_value.\n",
    "    '''\n",
    "    for index in indices:\n",
    "        df[new_col].iloc[index] = df[new_col].loc[index] + ' ' + df[old_col].loc[index]\n",
    "        df[old_col].loc[index] = new_value\n",
    "        \n",
    "\n",
    "transfer_and_replace_col('NOTARY', 'DOCNO', -1, need_transfer_index_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "679b4fb4-26f2-46c7-a217-4ad6caf9e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47281       Sp. W. Fla Vol. XI p.252\n",
       "47282       Sp. W. Fla Vol. XI p.252\n",
       "47283       Sp. W. Fla Vol. XI p.252\n",
       "47284       Sp. W. Fla Vol. XI p.252\n",
       "47619            SPWFLA Vol. 14 p. 1\n",
       "47620            SPWFLA Vol. 14 p. 1\n",
       "47621            SPWFLA Vol. 14 p. 1\n",
       "47622            SPWFLA Vol. 14 p. 1\n",
       "47623            SPWFLA Vol. 14 p. 1\n",
       "49165     West Fla Papers XII p. 142\n",
       "49357     Sp. W. Fla Vol. XII p. 222\n",
       "49358     Sp. W. Fla Vol. XII p. 222\n",
       "49359     Sp. W. Fla Vol. XII p. 222\n",
       "49360     Sp. W. Fla Vol. XII p. 222\n",
       "49361     Sp. W. Fla Vol. XII p. 222\n",
       "49362     Sp. W. Fla Vol. XII p. 222\n",
       "49363     Sp. W. Fla Vol. XII p. 222\n",
       "49364     Sp. W. Fla Vol. XII p. 222\n",
       "51120    Sp. W. Fla Vol. XIII p. 131\n",
       "51121    Sp. W. Fla Vol. XIII p. 131\n",
       "51122    Sp. W. Fla Vol. XIII p. 131\n",
       "51123    Sp. W. Fla Vol. XIII p. 131\n",
       "51124    Sp. W. Fla Vol. XIII p. 131\n",
       "51125    Sp. W. Fla Vol. XIII p. 131\n",
       "51126    Sp. W. Fla Vol. XIII p. 131\n",
       "56819     Sp. W. Fla Vol. XII p. 242\n",
       "Name: NOTARY, dtype: string"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOTARY'].loc[need_transfer_index_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "526778cc-ea9d-4ec3-b680-a6e7dc213913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47281    -1\n",
       "47282    -1\n",
       "47283    -1\n",
       "47284    -1\n",
       "47619    -1\n",
       "47620    -1\n",
       "47621    -1\n",
       "47622    -1\n",
       "47623    -1\n",
       "49165    -1\n",
       "49357    -1\n",
       "49358    -1\n",
       "49359    -1\n",
       "49360    -1\n",
       "49361    -1\n",
       "49362    -1\n",
       "49363    -1\n",
       "49364    -1\n",
       "51120    -1\n",
       "51121    -1\n",
       "51122    -1\n",
       "51123    -1\n",
       "51124    -1\n",
       "51125    -1\n",
       "51126    -1\n",
       "56819    -1\n",
       "Name: DOCNO, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DOCNO'].loc[need_transfer_index_only]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106cb76-b0a1-463e-bb1a-b5b3f48dff6d",
   "metadata": {},
   "source": [
    "I will have to record somewhere that document number -1 is a transfer to the notary column. Now that we have removed the page numbers, it's time to actually convert everything to a float in the DOCNO column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18afd3ca-cba4-4b62-9090-5d764e11937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOCNO'] = df['DOCNO'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39070ffc-6860-44a3-80bc-a48353e619f0",
   "metadata": {},
   "source": [
    "One of the things about hindsight in data analysis and software engineering is that when you can modularize and abstract your ideas, they appear both timeless and obvious. In reality, I manually changed a lot of values because I wasn't sure what the data looked like. Then I wrote a few prototype methods to automate the manual corrections. Now, I have a must more elegant solution that doesn't require nearly so much manual work or so much code. That being said, it should be noted that several hours of work, and several methods under development, have been essentially erased from this notebook. Which is cool, I suppose.\n",
    "\n",
    "I went to a lot of work to get DOCNO cleaned. Being a lazy son of a bitch, I decided to write a method that would attempt to do this for me and handle a few of the more common exceptions. Whether I wasted my time reinventing the wheel is undecided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c6cc886-361d-4259-b2d2-733ec8eec579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_and_cast(type_data, col):\n",
    "    '''\n",
    "    Takes a type and attempts to cast a column as that type. If this errors, it \n",
    "    attempts to replace the offending data as a sequence of decreasing negative\n",
    "    numbers.\n",
    "    '''\n",
    "    try:\n",
    "        if type_data == 'float':\n",
    "            df[col] = df[col].astype('float')\n",
    "        elif type_data == 'int':\n",
    "            df[col] = df[col].astype('int')\n",
    "        elif type_data == 'string':\n",
    "            df[col] = df[col].astype('string')\n",
    "        elif type_data == 'date':\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        else:\n",
    "            df[col] = df[col].astype('string')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        message = str(e)\n",
    "        if 'p.' in message:\n",
    "            print(\"It seems like there are page numbers in a column you want to cast as a float or int.\")\n",
    "            new_col = input(\"What column should page numbers be transferred to?\")\n",
    "            new_reference = input(\"What should be the new value in your current column?\")\n",
    "            transfer_and_replace(new_col, col, new_reference, [index for index, value in search_for_string_in_column_entry('.','DOCNO')])\n",
    "        return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376bc1c-96a3-4a51-a7a4-babad519e298",
   "metadata": {},
   "source": [
    "The above method will probably be most useful for further development of the project with new datasets. Nonetheless, I am going to continue on with the current dataset. As a reminder, we still have a few columns that should NOT be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67534def-c634-477d-9fbd-0435b2d8a95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOCNO', 'ARRIVEDATE', 'AGE', 'DATESALE', 'DATEINV']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e3a0f-58bf-421c-8cd7-ab0d622c3982",
   "metadata": {},
   "source": [
    "Now, however, all of the manual cleaning can be accomplished by the replace_and_cast method. As you can see, I was able to cast the ARRIVEDATE column as a date without any fuss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e758e17-9f12-4657-b8cc-638adc1aec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(replace_and_cast('date','ARRIVEDATE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d13840b2-e06c-4853-9528-52894a0fd44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ARRIVEDATE'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c7701-02c4-4d29-8efb-a26d98634068",
   "metadata": {},
   "source": [
    "Things got a bit trickier with the AGE column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70822345-5480-4565-871c-f09e90af6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot convert non-finite values (NA or inf) to integer\n"
     ]
    }
   ],
   "source": [
    "print(replace_and_cast('int','AGE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8652a-8629-47d2-90d7-f15f6221ff96",
   "metadata": {},
   "source": [
    "However, the solution here is to remember that the NaN character is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd7f2c88-22bb-4daa-8ee0-ea49f771d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE'] = df['AGE'].fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85f05089-e78e-4537-9fa7-f3b94fa6173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGE'] = df['AGE'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c1cd0ae-4ba6-4ceb-bc15-43fce36311d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AGE'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a2a8d-8d8c-4616-bd73-3be1a2096db8",
   "metadata": {},
   "source": [
    "There was one final issue with the DATEINV and DATESALE columns. This was one of those fascinating errors that honestly tells me a lot about the prejudices (probably unintentional) of the developers of Pandas. See, as I've said before, the library was developed for scientific and mathematical computation. Data scientists use it for non-numeric data, but generally for contemporary data for tech companies selling ad revenue to oligarchies. It probably never occurred to the developers that Pandas could be used to study, say, the 14th century.\n",
    "\n",
    "Time is currently recorded in Pandas as a record of nanoseconds preceding from a specific point. I believe this is the same as the generic system time used in Unix computers, which point to 1 January, 1970 as \"Day 0\". Once you begin counting forwards form 1970, Pandas keeps track of where you are, and converts between different dates, based on the number of nanoseconds since 1 January, 1970. The same is true but in reverse when going backwards.\n",
    "\n",
    "The developers of Pandas generously gave the nanosecond parameter a 64 bit integer length. This means that there are 9,223,372,036,854,775,808 possible nanoseconds that you can proceed into the past. That's actually one more nanosecond that you can proceed into the future, because 0 is considered a future nanosecond. This is a total of 292 years of backwards compatibility. Due to some clever maths, you can extend this to 584 years. And then the computer remembers that 584 years is a really, really big number when counted in nanoseconds, and runs out of memory.\n",
    "\n",
    "For those who remember the fears about Y2K, it's like that. But, this time, it doesn't really matter so long as we tell the computer it is ok to count the years in a different way. For this project, I'm able to assume that any date before 1719 is invalid. Slaves didn't appear in Louisiana before 1719. Even if I were to extend the project to \"Anyone of Eurasian or African descent\" I would only be getting dates in the very late 1400s. 584 years ago was before Europeans realized North America existed. So, for now, I can safely assume that any data older than 1700 is in error.\n",
    "\n",
    "In the below method, I simply add 1000 years to any date from before 1700. I'll have to change that year for states like Virginia, of course, which had the first slaves land in 1619. \n",
    "\n",
    "An example of the DATESALE error is below. DATESALE seems to have a far simpler solution than DATEINV. Simply put, someone keyed in a 1404 when they meant 1804."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbcdfd07-6a00-4a82-a845-a9b540443f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATESALE'] = pd.to_datetime(df['DATESALE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf477a1-f9c0-46b4-9354-5bf72e976c4f",
   "metadata": {},
   "source": [
    "The DATESALE table can be seen below. I've selected only the columns with dates older than 1700. Notice that althouth the DATESALE date is 1404, the DOCDATE is 1804. This looks like input error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5e54aab-174c-48d2-ad77-6fac86ba636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datesale_problems = [index for index, value in df['DATESALE'].items() if value is not None and value < pd.to_datetime('1701-01-01')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77e9afe6-a6a7-4570-ae8f-adea20f5215c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCDATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>DOCNO</th>\n",
       "      <th>NOTARY</th>\n",
       "      <th>CODER</th>\n",
       "      <th>DATEINV</th>\n",
       "      <th>DATESALE</th>\n",
       "      <th>DEPOT</th>\n",
       "      <th>PARISH</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>...</th>\n",
       "      <th>ENTERPRISE</th>\n",
       "      <th>CAPTAIN</th>\n",
       "      <th>SLAVETRADE</th>\n",
       "      <th>STPORT</th>\n",
       "      <th>SHIP</th>\n",
       "      <th>ARRIVEDATE</th>\n",
       "      <th>FROM</th>\n",
       "      <th>UNBAPT</th>\n",
       "      <th>VIA</th>\n",
       "      <th>COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DOCDATE, YEAR, DOCNO, NOTARY, CODER, DATEINV, DATESALE, DEPOT, PARISH, LOCATION, DOCTYPE, REVOLTS, MAROON, LANGUAGE, LINGUISTIC, ESTFREE, FREE, ESTATE_OF, FIRSTNAME, ESTATE, ESTATEPOP, SELLER, FIRST1, BUYER, FIRST2, WENT, NAMEXPLAIN, NAME, NAMETYPE, ISLAMIC, SEX, RACE, AGE, AGECATN, SKILLS, SKILLCAT, EXPERT, APPRENTICE, ALITTLE, SKILL2, SKILL3, SKILL4, SKILL5, CHARACTER, CHARCAT, CHARCAT2, CHARCAT3, SICK, SICKCAT, SICK2, SICK3, SICK4, SPELL, BIRTHPL, AFLANG, BRUT, GROUP, GROUPMEMB, INVCUR, INVVALUE, INVVALP, SALECUR, SALEVALUE, SALEVALP, FAMILY_Y_N, FAMILY, CHILDREN, MALE, FEMALE, UNDER5, PREGNANT, MOTHER, AGEMOM, RACEMOM, INVWMOM, SOLDWMOM, SPNATMOM, ORMOM, MATE, MATENAME, AGEMATE, AGECATMATE, RACEMATE, SPELNAMATE, ORIGINMATE, FATHER, AGEDAD, RACEDAD, SPNADAD, ORDAD, GRANDCHILD, GRANDSONS, GDAUGHTERS, GRANDMA, AGEGRANDMA, SPNAGM, ORGRANDMA, GRANPA, AGEGRANDPA, SPNAGRPA, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 114 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[datesale_problems]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92584d-eb4a-4c17-a93f-a577b9901049",
   "metadata": {},
   "source": [
    "The simplest solution here would be to replace the dates in the DATESALE column with those in the DOCDATE column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5488cdab-36f0-463c-9359-232573632c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_to_col_dates(from_col, to_col, limit_date):\n",
    "    '''\n",
    "    Changes the to_col values to match those of the from_col\n",
    "    \n",
    "    Only used for datetime objects\n",
    "    '''\n",
    "    for index, value in df[to_col].items():\n",
    "        if value is None or value < pd.to_datetime(limit_date):\n",
    "        \n",
    "            df[to_col].iloc[index] = df[from_col].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b30c3810-0fcc-40b2-8495-2816d29d146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_col_to_col_dates('DOCDATE', 'DATEINV', '1701-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d77fe573-ad75-497a-b712-aaed31a7b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_col_to_col_dates('DOCDATE', 'DATESALE', '1701-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "159012e7-0702-4ec9-a71f-f0eab2f91faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE                                     1719-06-15 00:00:00\n",
       "YEAR                                                       1719\n",
       "DOCNO                                                       NaN\n",
       "NOTARY                                                         \n",
       "CODER                                                       1.0\n",
       "                                    ...                        \n",
       "ARRIVEDATE                                           1719-06-15\n",
       "FROM                                       Juda (Bight of Benin\n",
       "UNBAPT                                                      NaN\n",
       "VIA                                                         NaN\n",
       "COMMENTS      One of first two ships. June 1719 arrival date...\n",
       "Name: 0, Length: 114, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b7852f9-4415-4627-b715-c1a1ee9c8c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE                                     1719-06-15 00:00:00\n",
       "YEAR                                                       1719\n",
       "DOCNO                                                       NaN\n",
       "NOTARY                                                         \n",
       "CODER                                                       1.0\n",
       "                                    ...                        \n",
       "ARRIVEDATE                                           1719-06-15\n",
       "FROM                                       Juda (Bight of Benin\n",
       "UNBAPT                                                      NaN\n",
       "VIA                                                         NaN\n",
       "COMMENTS      One of first two ships. June 1719 arrival date...\n",
       "Name: 0, Length: 114, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b1b8917-9611-4dff-9f1f-d4953ac5fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATESALE'] = df['DATESALE'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "573d87cd-c984-4923-8516-53d96956a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATESALE'] = pd.to_datetime(df['DATESALE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ac32244-e78e-4c19-9002-7f6d7a32d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATEINV'] = df['DATEINV'].reset_index()\n",
    "df['DATEINV'] = pd.to_datetime(df['DATEINV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94b577e8-bd18-4d10-bda0-be06bfc5ede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DATEINV'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6aa30515-2acf-45e8-b6db-9e4e14cbb518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DATESALE'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54c2cf41-a4f0-4233-86ff-0014f9d47db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCDATE       datetime64[ns]\n",
       "YEAR                   int64\n",
       "DOCNO                 object\n",
       "NOTARY                string\n",
       "CODER                float64\n",
       "                   ...      \n",
       "ARRIVEDATE            object\n",
       "FROM                  string\n",
       "UNBAPT               float64\n",
       "VIA                  float64\n",
       "COMMENTS              string\n",
       "Length: 114, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088a403-ac92-49ec-90df-1f9952cf9e9f",
   "metadata": {},
   "source": [
    "Pandas has a generic 'object' type. This is usually bad, since it can mean mixed types of data. Let's make sure there's no data with the 'object' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d12039b-df7c-418c-9627-1653fd2b5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5434e721-89e5-45c2-9347-98c3372a97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_entries():\n",
    "    '''\n",
    "    Print out the names of all columns in the dataframe\n",
    "    '''\n",
    "    return [entries for entries in df]\n",
    "\n",
    "list_of_entries = print_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380363e-3853-4352-8b45-a585055da11c",
   "metadata": {},
   "source": [
    "Let's get a handle on each of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed7c98e8-929d-424c-ba58-a6a9862cfaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOCDATE'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_entries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d46bac-1c5c-4774-aba7-5285e75b9ea4",
   "metadata": {},
   "source": [
    "What can we learn about DOCDATE? Well, for one thing, we should probably know if there are undated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ed7982e0-d4a3-472f-851f-b644fd03fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_docdates = [value for index, value in df['DOCDATE'].items() if value == \"NaN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "893fbefe-a515-4f65-a172-6b70dcf18691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_docdates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8cbd7-81f4-4274-8ab6-29f97d46df6b",
   "metadata": {},
   "source": [
    "Looks like we don't really have any blank dates in DOCDATE. That's a good sign. It's the column we are relying on for all of the other dates. Let's see if we can get an idea of how evenly documents are distributed over the time period. Actually, first, let's see our maximum and minimum dates so we know what time period we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a4a8feec-5aff-49e3-b178-35d03b028c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docdate = df['DOCDATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c9d2d312-5823-4cf5-b9a9-69494d021eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1820-12-31 00:00:00')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdate.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b723e8-ac51-4803-96c3-631490f630f8",
   "metadata": {},
   "source": [
    "So, the data ends in 1820. It ends strangely enough on the 31st of December, leading me to wonder why records stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "17b20d08-71b2-4097-8055-b1e6ffed1cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1719-06-15 00:00:00')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdate.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c58fc-9975-4e28-81f9-db628b70de90",
   "metadata": {},
   "source": [
    "The beginning of the period, then, is 1719. We are exploring 101 years of slavedealing history. But as with most things in history, there's a slight problem. What happens if we take the median of DOCDATE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "688893a5-b70d-47be-b7d2-d5f2f5b317c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1808-05-04 00:00:00')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdate.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ead78-4c80-412e-965d-e6f111ccaf9d",
   "metadata": {},
   "source": [
    "The median is the point where 50% of the documents are dated before and 50% dated after the median date. What, then, is the median date? 1808. 50% of the slaveholding documents were produced in 12 years, and 50% in 89 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f4fcc3bf-790d-488e-8e74-19644204eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1786-12-31\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdate.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe83bf-f371-4d59-8a1e-c5d7245ff73b",
   "metadata": {},
   "source": [
    "Strangely enough, the most common DOCDATE entry is 1786-12-31. How many such docdate entries exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f748f717-1f43-44c8-9ed7-64e1f4325059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_value(series, desired):\n",
    "    counter = 0\n",
    "    for index, value in series.items():\n",
    "        if value == desired:\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "79e6c938-7b13-4813-93d2-e069532dbf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_for_value(docdate, pd.to_datetime('1786-12-31'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565894ef-5bfb-42b8-9ca4-45a18e490e2a",
   "metadata": {},
   "source": [
    "WOAH! 957 different records come from 1786, on the last day of the year. I have no clue why, at the moment. This points to some very strange issues with the data. How many records are clustered on a given day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9fd67e49-09cf-4a84-97f3-68b027fbcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docdate_occurrences = docdate.value_counts().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b18bd3a9-a72c-46a7-9f7f-ace93598bc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdate_occurrences.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f7e37-69b0-45bc-b987-d36b6ad33892",
   "metadata": {},
   "source": [
    "Based on the median value in the occurrences column, I can conclude that most of these documents record small transactions. This is, I suppose, not very surprising. I would suspect most slaveholding transactions were oddly personal for something so dehumanizing. Probably if you plotted the sale of dogs or cows even in the present day you'd see the same disparity between small transactions and gigantic transactions.\n",
    "\n",
    "Let's get a good look at these larger and smaller transactions by separating them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f99df67c-f4fc-4238-93d1-e1e44dec434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_five = []\n",
    "five_above = []\n",
    "for index, values in docdate_occurrences.items():\n",
    "    if values < 5:\n",
    "        sub_five.append(values)\n",
    "    else:\n",
    "        five_above.append(values)\n",
    "below_five = pd.Series(sub_five)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "db5801cd-c1d2-4e8d-92aa-e627b768195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "above_four = pd.Series(five_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "22b122be-4b42-4dcf-87b0-5079a966e126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       4\n",
       "2       4\n",
       "3       4\n",
       "4       4\n",
       "       ..\n",
       "6378    1\n",
       "6379    1\n",
       "6380    1\n",
       "6381    1\n",
       "6382    1\n",
       "Length: 6383, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a303b82-ab40-463f-b909-3e79b7a1901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       957\n",
       "1       597\n",
       "2       500\n",
       "3       464\n",
       "4       451\n",
       "       ... \n",
       "4904      5\n",
       "4905      5\n",
       "4906      5\n",
       "4907      5\n",
       "4908      5\n",
       "Length: 4909, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd19d1-f748-474d-9348-b1c587418177",
   "metadata": {},
   "source": [
    "There are 4,908 discrete dates and documents with above five listings. Meanwhile, there are  6,383 discrete dates and documents with below four listings. This means there are really quite a few more small transactions. I suspect there are simply a few enormous transactions and a bunch of small to medium transactions. Let's further divide the group of above five transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bba1092e-25b0-491c-a7ab-a3bde544cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_four.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fdc50-5452-449a-99e5-77dc29304155",
   "metadata": {},
   "source": [
    "Now we can tell that half of the 4,908 records in the above four group are in the 5-10 humans being sold range. As suspected, there are still small transactions being listed with the enormous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "81f7950a-95c2-45e4-87f1-45a1314aaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_eleven = []\n",
    "eleven_or_greater = []\n",
    "for index, values in docdate_occurrences.items():\n",
    "    if values < 11:\n",
    "        sub_eleven.append(values)\n",
    "    else:\n",
    "        eleven_or_greater.append(values)\n",
    "below_eleven = pd.Series(sub_eleven)\n",
    "eleven_above = pd.Series(eleven_or_greater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e6dc1e7f-7203-457f-b717-604245031cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_eleven.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "778ecb73-56c7-4217-bcd6-157f76889bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eleven_above.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "640f3248-4596-442d-822e-8ef15e303b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       957\n",
       "1       597\n",
       "2       500\n",
       "3       464\n",
       "4       451\n",
       "       ... \n",
       "2368     11\n",
       "2369     11\n",
       "2370     11\n",
       "2371     11\n",
       "2372     11\n",
       "Length: 2373, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eleven_above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f42282-101c-4ba9-9fa3-ee7f1061ef03",
   "metadata": {},
   "source": [
    "At what point does the sale of human beings become a large transaction? 10 humans? 100 humans? Somehow, I know I'm groping for some sort of manageable scale but I have to remember that these are human beings whose data I'm playing with. 2,373 people were sold in lots of 11 or more. This is both morally equivalent to selling them in lots of 1, and also somehow worse. But there is a part of me that thinks that sales in excess of 50 or 100 are even more horrendous. I feel like I can keep slicing up these transactions by the median, but the truth is that eventually I have to stop. I choose to stop when the median number of humans sold is 50. My mind can't really understand what it is like to line 50 human beings up and sell them like a horse. I'm not even sure I can imagine selling the horses like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1441ebc5-55ef-4fcb-a5b6-30f9fc5b0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_eighteen = []\n",
    "eighteen_or_greater = []\n",
    "for index, values in docdate_occurrences.items():\n",
    "    if values < 18:\n",
    "        sub_eighteen.append(values)\n",
    "    else:\n",
    "        eighteen_or_greater.append(values)\n",
    "below_eighteen = pd.Series(sub_eighteen)\n",
    "eighteen_above = pd.Series(eighteen_or_greater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d2b06d56-eed8-4cdf-a38d-df6cd67501ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_eighteen.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0764ef6e-d459-4522-84b0-8e4a7b1fa9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eighteen_above.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "20a4ca34-904d-44a0-b93f-e2fe881663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_28 = []\n",
    "twentyeight_or_greater = []\n",
    "for index, values in docdate_occurrences.items():\n",
    "    if values < 28:\n",
    "        sub_28.append(values)\n",
    "    else:\n",
    "        twentyeight_or_greater.append(values)\n",
    "below_twentyeight = pd.Series(sub_28)\n",
    "twentyeight_above = pd.Series(twentyeight_or_greater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "74d168fc-331f-45f6-be69-6e268bd7bec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_twentyeight.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "906192a7-b757-4ad8-846f-57ea7bd13b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twentyeight_above.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "303094ce-8e60-4f77-8429-2393f54c879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      957\n",
       "1      597\n",
       "2      500\n",
       "3      464\n",
       "4      451\n",
       "      ... \n",
       "671     28\n",
       "672     28\n",
       "673     28\n",
       "674     28\n",
       "675     28\n",
       "Length: 676, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twentyeight_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a459959-652e-438d-a21a-d510bf84bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
